{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Big data Competition</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta=r\"C:\\Users\\lordg\\Desktop\\Jacobs resourses\\big_data_comp\"\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_csv(ruta+'\\\\train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Let's build 22 PCA mapping e.g. 1 per label.</h2>\n",
    "<p>We try to reduce dimensions. Usume 42 is got number of principal vector per PCA</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curve-left-step\n",
      "curve-left-step number of rows 299\n",
      "stand-to-sit\n",
      "stand-to-sit number of rows 289\n",
      "curve-right-spin-Rfirst\n",
      "curve-right-spin-Rfirst number of rows 300\n",
      "jump-one-leg\n",
      "jump-one-leg number of rows 279\n",
      "lateral-shuffle-right\n",
      "lateral-shuffle-right number of rows 277\n",
      "curve-right-spin-Lfirst\n",
      "curve-right-spin-Lfirst number of rows 301\n",
      "v-cut-right-Lfirst\n",
      "v-cut-right-Lfirst number of rows 300\n",
      "stair-down\n",
      "stair-down number of rows 278\n",
      "v-cut-left-Rfirst\n",
      "v-cut-left-Rfirst number of rows 280\n",
      "v-cut-right-Rfirst\n",
      "v-cut-right-Rfirst number of rows 280\n",
      "jump-two-leg\n",
      "jump-two-leg number of rows 280\n",
      "sit\n",
      "sit number of rows 289\n",
      "stair-up\n",
      "stair-up number of rows 278\n",
      "curve-right-step\n",
      "curve-right-step number of rows 295\n",
      "sit-to-stand\n",
      "sit-to-stand number of rows 289\n",
      "run\n",
      "run number of rows 300\n",
      "v-cut-left-Lfirst\n",
      "v-cut-left-Lfirst number of rows 300\n",
      "stand\n",
      "stand number of rows 289\n",
      "curve-left-spin-Lfirst\n",
      "curve-left-spin-Lfirst number of rows 280\n",
      "walk\n",
      "walk number of rows 300\n",
      "curve-left-spin-Rfirst\n",
      "curve-left-spin-Rfirst number of rows 320\n",
      "lateral-shuffle-left\n",
      "lateral-shuffle-left number of rows 282\n",
      "lay\n",
      "lay number of rows 16\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "labels=list(df_train.Label.unique())\n",
    "#labels='stand-to-sit'\n",
    "models=[]\n",
    "\n",
    "for each in labels:\n",
    "    print(each)\n",
    "    pca = PCA (n_components =275)\n",
    "    df=df_train[df_train['Label']==each]\n",
    "    files=list(df.Datafile)\n",
    "    arr={}\n",
    "    col_sub='EMG1,EMG2,EMG3,EMG4,Airborne,ACC_upper_X,ACC_upper_Y,ACC_upper_Z,Goniometer_X,ACC_lower_X,ACC_lower_Y,ACC_loewr_Z,Goniometer_Y,Gyro_upper_X,Gyro_upper_Y,Gyro_upper_Z,Gyro_lower_X,Gyro_lower_Y,Gyro_lower_Z'\n",
    "    j=0\n",
    "    for each_file in files:\n",
    "        #print(each_file)\n",
    "        df_explo=pd.read_csv(ruta+'\\\\Subjects\\\\'+each_file.replace('/','\\\\'),names=col_sub.split(','))\n",
    "        #print(each_file,each)\n",
    "        try:\n",
    "            x=list(df_explo.values[10:3000].reshape((1, 56810))[0]) \n",
    "        except:\n",
    "            #complete by repetition\n",
    "            df_temp=df_explo.append(df_explo).append(df_explo).append(df_explo)\n",
    "            x=list(df_temp.values[10:3000].reshape((1, 56810))[0])\n",
    "        arr[j]=x\n",
    "        j=j+1\n",
    "    for_pca=pd.DataFrame.from_dict(arr, orient='index')\n",
    "    \n",
    "    print(each,'number of rows',j)\n",
    "    if(each=='lay'):\n",
    "        pca = PCA (n_components =16)\n",
    "        \n",
    "    models.append(pca.fit(for_pca))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Using pickle, we can save python object on our hard drive, for later use</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From saving the object models as a file for later usage\n",
    "import pickle\n",
    "with open(\"models_pca.file\", \"wb\") as f:\n",
    "    pickle.dump(models, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#calling from hard drive\n",
    "with open(\"models_pca.file\", \"rb\") as f:\n",
    "    models_pca = pickle.load(f)\n",
    "    # Now you can use the dump object as the original one  \n",
    "    #self.some_property = dump.some_property"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Using our 22 PCA, we train 22  logistic regression models. Here we use 10 fold cross validation for estimating the test error and to estimate the regularization parameter C. We also use f1_score to measure performance.</h1>\n",
    "<p> each of the these model will give us an estimation of the probability of the one time series for instance to belongs or not to a particular class. For example letâ€™s say that the MODEL(PCA_1) finds a 90%  probability that it belongs to the class 'stand', if we compare this against the second model, and we see  that it would give us 85% probability of that time series being of class 'run', then we would conclude that it belongs to the class 'stand' since 90%>85%  </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier for  curve-left-step  ready. X_shape: (6401, 275)\n",
      "classifier for  stand-to-sit  ready. X_shape: (6401, 275)\n",
      "classifier for  curve-right-spin-Rfirst  ready. X_shape: (6401, 275)\n",
      "classifier for  jump-one-leg  ready. X_shape: (6401, 275)\n",
      "classifier for  lateral-shuffle-right  ready. X_shape: (6401, 275)\n",
      "classifier for  curve-right-spin-Lfirst  ready. X_shape: (6401, 275)\n",
      "classifier for  v-cut-right-Lfirst  ready. X_shape: (6401, 275)\n",
      "classifier for  stair-down  ready. X_shape: (6401, 275)\n",
      "classifier for  v-cut-left-Rfirst  ready. X_shape: (6401, 275)\n",
      "classifier for  v-cut-right-Rfirst  ready. X_shape: (6401, 275)\n",
      "classifier for  jump-two-leg  ready. X_shape: (6401, 275)\n",
      "classifier for  sit  ready. X_shape: (6401, 275)\n",
      "classifier for  stair-up  ready. X_shape: (6401, 275)\n",
      "classifier for  curve-right-step  ready. X_shape: (6401, 275)\n",
      "classifier for  sit-to-stand  ready. X_shape: (6401, 275)\n",
      "classifier for  run  ready. X_shape: (6401, 275)\n",
      "classifier for  v-cut-left-Lfirst  ready. X_shape: (6401, 275)\n",
      "classifier for  stand  ready. X_shape: (6401, 275)\n",
      "classifier for  curve-left-spin-Lfirst  ready. X_shape: (6401, 275)\n",
      "classifier for  walk  ready. X_shape: (6401, 275)\n",
      "classifier for  curve-left-spin-Rfirst  ready. X_shape: (6401, 275)\n",
      "classifier for  lateral-shuffle-left  ready. X_shape: (6401, 275)\n",
      "classifier for  lay  ready. X_shape: (6401, 16)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "df_train=pd.read_csv(ruta+'\\\\train.csv')\n",
    "import numpy as np\n",
    "#Data_PCA=model[0].transform(Data)\n",
    "\n",
    "\n",
    "labels=list(pd.read_csv(ruta+'\\\\train.csv').Label.unique())\n",
    "\n",
    "index_label=0\n",
    "\n",
    "#here we our 22 models.\n",
    "list_models=[]\n",
    "\n",
    "for each in labels:\n",
    "    df_train=pd.read_csv(ruta+'\\\\train.csv')\n",
    "    #df=df_train[df_train['Label']==each]\n",
    "    df_train['Label'] = df_train['Label'].apply(lambda x: 1 if x==each else 0)\n",
    "    l_df=list(df_train.values)\n",
    "    files=list(df_train.Datafile)\n",
    "    arr_trans={}\n",
    "    col_sub='EMG1,EMG2,EMG3,EMG4,Airborne,ACC_upper_X,ACC_upper_Y,ACC_upper_Z,Goniometer_X,ACC_lower_X,ACC_lower_Y,ACC_loewr_Z,Goniometer_Y,Gyro_upper_X,Gyro_upper_Y,Gyro_upper_Z,Gyro_lower_X,Gyro_lower_Y,Gyro_lower_Z'\n",
    "    j=0\n",
    "    dict_val_X,dict_val_y={},{}\n",
    "    for each_file in files:\n",
    "        #print(each_file)\n",
    "        df_explo=pd.read_csv(ruta+'\\\\Subjects\\\\'+each_file.replace('/','\\\\'),names=col_sub.split(','))\n",
    "        #print(each_file,each)\n",
    "        try:\n",
    "            x=np.array(df_explo.values[10:3000].reshape((1, 56810))[0]) \n",
    "        except:\n",
    "            #complete by repetition\n",
    "            df_temp=df_explo.append(df_explo).append(df_explo).append(df_explo)\n",
    "            x=np.array(df_temp.values[10:3000].reshape((1, 56810))[0])\n",
    "        dict_val_X[j]=models_pca[index_label].transform(x.reshape(1, -1))[0]\n",
    "        dict_val_y[j]=l_df[j][2]\n",
    "        j=j+1\n",
    "        \n",
    "    X=np.array(list(dict_val_X.values()))\n",
    "    y=np.array(list(dict_val_y.values()))\n",
    "    f1_score_weighted = make_scorer(f1_score, average='weighted')\n",
    "    CV_LR = LogisticRegressionCV(cv=10,  Cs=[0.1,0.5, 1,1.5, 5,10,50, 100],scoring=f1_score_weighted)\n",
    "    #Este el objeto que nos interesa, este el modelo entranado.\n",
    "    cv_classifier_LR= CV_LR.fit(X, y)\n",
    "    list_models.append(cv_classifier_LR)\n",
    "    print('classifier for ',each, ' ready. X_shape:', X.shape)\n",
    "    index_label += 1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From saving the object models as a file for later usage\n",
    "import pickle\n",
    "with open(\"list_models.file\", \"wb\") as f:\n",
    "    pickle.dump(list_models, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "with open(\"list_models.file\", \"rb\") as f:\n",
    "    list_models = pickle.load(f)\n",
    "    # Now you can use the dump object as the original one  \n",
    "    #self.some_property = dump.some_property"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Finally we make predictions</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "labels=list(pd.read_csv(ruta+'\\\\train.csv').Label.unique())\n",
    "\n",
    "real_pred=[]\n",
    "df_test=pd.read_csv(ruta+'\\\\train.csv')\n",
    "files=list(df_test.Datafile)\n",
    "arr={}\n",
    "#ln_labels=list(df_test.Label)\n",
    "for each_data in files:\n",
    "    df_explo=pd.read_csv(ruta+'\\\\Subjects\\\\'+each_data.replace('/','\\\\'))\n",
    "    try:\n",
    "        x=np.array(df_explo.values[10:3000].reshape((1, 56810))[0]) \n",
    "    except:\n",
    "        #complete by repetition\n",
    "        df_temp=df_explo.append(df_explo).append(df_explo).append(df_explo)\n",
    "        x=np.array(df_temp.values[10:3000].reshape((1, 56810))[0])\n",
    "    \n",
    "    index_label=0\n",
    "    arr={}\n",
    "    for each_label in labels:\n",
    "        predi=models_pca[index_label].transform(x.reshape(1, -1))[0]\n",
    "        XX=list_models[index_label].predict_proba(predi.reshape(1, -1))\n",
    "        arr[index_label]=[XX[0][0]]\n",
    "        #larr[index_label]=[ln_labels[index_label]]\n",
    "        index_label += 1\n",
    "    #here we select the maximum probality of the array of class predictions probalities\n",
    "    \n",
    "    real_pred.append(arr)\n",
    "    \n",
    "    #real_pred.append(labels[max(arr.items(), key=operator.itemgetter(1))[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Let us reconstruct the test data for submission.</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dicrr={}\n",
    "\n",
    "#Subject_c=list(df_test.Subject.values)\n",
    "#dicrr['Subject']=Subject_c\n",
    "\n",
    "#Datafile_c=list(df_test.Datafile.values)\n",
    "#dicrr['Datafile']=Datafile_c\n",
    "\n",
    "#dicrr['Label_pre']=real_pred\n",
    "\n",
    "#Subject_c=list(df_test.Label.values)\n",
    "#dicrr['Label']=Subject_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#real_pred\n",
    "\n",
    "X_train_dic={}\n",
    "y_train_dic={}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dic['Label_predicted_array']=real_pred\n",
    "\n",
    "fuck={}\n",
    "nn=0\n",
    "al=[]\n",
    "for each in real_pred:\n",
    "    #print(list(each.values()))\n",
    "    #nn=0\n",
    "    for cada in list(each.values()):\n",
    "        al.append(cada[0])\n",
    "    fuck[nn]=al\n",
    "    nn=nn+1\n",
    "    al=[]\n",
    "    \n",
    "#data=np.array(fuck)\n",
    "df = pd.DataFrame(fuck)  \n",
    "#fuck\n",
    "#fuck[1]\n",
    "r_df=df.T\n",
    "r_df.columns=labels\n",
    "\n",
    "classes=list(df_test.Label.values)\n",
    "y_train_dic['Target']=classes\n",
    "#dicrr['real']=real_pred\n",
    "#X_train_dic['Label_predicted_array']=real_pred\n",
    "#Subject,Datafile,Label\n",
    "\n",
    "data_X_train=r_df\n",
    "data_y_train=pd.DataFrame.from_dict(y_train_dic)   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#splitting 75% train, 25% test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#From saving the object models as a file for later usage\n",
    "import pickle\n",
    "with open(\"data_X_train.file\", \"wb\") as f:\n",
    "    pickle.dump(data_X_train, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(\"data_y_train.file\", \"wb\") as f:\n",
    "    pickle.dump(data_y_train, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    \n",
    "with open(\"data_X_train.file\", \"rb\") as f:\n",
    "    data_X_train = pickle.load(f)\n",
    "    # Now you can use the dump object as the original one  \n",
    "    #self.some_property = dump.some_property\n",
    "\n",
    "with open(\"data_y_train.file\", \"rb\") as f:\n",
    "    data_y_train = pickle.load(f)\n",
    "    # Now you can use the dump object as the original one  \n",
    "    #self.some_property = dump.some_property\n",
    "    \n",
    "#random_state is the seed of the random selection, use the same to ensure reproducibility.\n",
    "X_train,X_test,y_train,y_test = train_test_split(data_X_train,data_y_train,test_size=0.25,random_state=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we import the class tree from sklearn\n",
    "\n",
    "from sklearn import tree\n",
    "\n",
    "#Create the object model.\n",
    "tree_model = tree.DecisionTreeClassifier()\n",
    "#Fit the model.\n",
    "tree_model = tree_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#This class would allow us to draw the decision Tree.\n",
    "#from io import StringIO\n",
    "#import pydot \n",
    "#import graphviz \n",
    "#dot_data = tree.export_graphviz(tree_model, out_file=None,\n",
    "#                                feature_names=X_train.columns, \n",
    "#                                class_names=classes, \n",
    "#                                filled=True,\n",
    "#                                special_characters=True)  \n",
    "#graph = graphviz.Source(dot_data)  \n",
    "#Show inline in jupyter notebook\n",
    "#graph \n",
    "\n",
    "#For saving as pdf\n",
    "#dot_data = StringIO() \n",
    "#tree.export_graphviz(tree_model, \n",
    "#                     out_file=dot_data,\n",
    "#                     feature_names=X_train.columns, \n",
    "#                     class_names=classes, \n",
    "#                     filled=True,\n",
    "#                     special_characters=True) \n",
    "#graph = pydot.graph_from_dot_data(dot_data.getvalue()) \n",
    "#graph[0].write_pdf(\"tree_challange.pdf\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Here create the martrix of prediction over the test sets.\n",
    "y_pred_test=tree_model.predict(X_test)\n",
    "\n",
    "#Here create the martrix of prediction over the train sets.\n",
    "y_pred_train=tree_model.predict(X_train)\n",
    "\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "np.set_printoptions(precision=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metrics definition.\n",
    "def accuracy(confusion_matrix):\n",
    "    sum_diagonal=sum(np.diagonal(confusion_matrix))\n",
    "    total=sum(sum(confusion_matrix))\n",
    "    return (sum_diagonal/total)\n",
    "\n",
    "def precision(label, confusion_matrix):\n",
    "    col = confusion_matrix[:, label]\n",
    "    return confusion_matrix[label, label] / col.sum()\n",
    "    \n",
    "def recall(label, confusion_matrix):\n",
    "    row = confusion_matrix[label, :]\n",
    "    return confusion_matrix[label, label] / row.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall TEST accuracy\n",
      "-----> 0.7601499063085572\n",
      "------------------------------------------------\n",
      "Overall TRAIN accuracy\n",
      "-----> 1.0\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Overall TEST accuracy\")\n",
    "print(\"----->\",accuracy(cnf_matrix))\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "print(\"Overall TRAIN accuracy\")\n",
    "print(\"----->\",accuracy(confusion_matrix(y_train, y_pred_train)))\n",
    "print(\"------------------------------------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>let us try random forest</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boosting:\n",
      "\n",
      "Overall TEST accuracy\n",
      "-----> 0.8107432854465959\n",
      "------------------------------------------------\n",
      "Overall TRAIN accuracy\n",
      "-----> 0.9995833333333334\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier \n",
    "\n",
    "\n",
    "#instate the object model.\n",
    "boosting_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.45, max_depth=7, random_state=0)\n",
    "#Fit the model.\n",
    "boosting_model = boosting_model.fit(X_train, y_train)\n",
    "\n",
    "#Here create the martrix of prediction over the test sets.\n",
    "y_pred_test=boosting_model.predict(X_test)\n",
    "#Here create the martrix of prediction over the train sets.\n",
    "y_pred_train=boosting_model.predict(X_train)\n",
    "\n",
    "cnf_boosting_test_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "cnf_boosting_train_matrix = confusion_matrix(y_train, y_pred_train)\n",
    "\n",
    "print(\"boosting:\\n\")\n",
    "print(\"Overall TEST accuracy\")\n",
    "print(\"----->\",accuracy(cnf_boosting_test_matrix))\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "print(\"Overall TRAIN accuracy\")\n",
    "print(\"----->\",accuracy(cnf_boosting_train_matrix))\n",
    "print(\"------------------------------------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicrr={}\n",
    "\n",
    "Subject_c=list(df_test.Subject.values)\n",
    "dicrr['Subject']=Subject_c\n",
    "\n",
    "Datafile_c=list(df_test.Datafile.values)\n",
    "dicrr['Datafile']=Datafile_c\n",
    "\n",
    "dicrr['Label_pre']=y_pred_test\n",
    "\n",
    "Subject_c=list(df_test.Label.values)\n",
    "dicrr['Label']=Subject_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame_out=pd.DataFrame.from_dict(dicrr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_frame_out.to_csv(index=False,path_or_buf=ruta+'test_03_04.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Example diagram</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aquiiiii"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
